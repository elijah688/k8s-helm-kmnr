# Utility Container

**NOTE:** All artefacts we're about to create can be found in `/kubernetes/helm`.

Firstly, in order to be able to play around with `kubernetes`, we need a couple of tool, i.e. `kubectl` and `helm`. We'll install them and run them in a docekr container. To do this, we can run the `elijah668/utils` image or build and run the `util.dockerfile`.

**NOTE**: When running either image, enable `tty` and `stdin`.

We can bootstrap all that by bringing up a container with the following docker-compose command:

```console
docker-compose -f util.yaml up -d
```

To bring the container down we can do:

```console
docker-compose -f util.yaml down
```

To access our running container we do:

```console
 docker container exec -it util /bin/sh
```

# Kubectl Config

Next we need to point `kubectl` to our cluster. The way we do that is by placing our cluster config file in the `/root/.kube` directory in the running util container.

**NOTE**: If you're following along on a docker desktop local cluster, you can add a bind mount to `/Users/yourusername/.kube/config`. To do this we simply add the the following to the docker-compose.yaml file:

```yaml
services: ...
volumes:
  - ${HOME}/.kube:/root/.kube:ro
```

To verify if kubectl can interface the cluster, we exec back into the util container and run:

```console
 kubectl version
```

# Add a Helm Chart

Firstly, we'll create a `services` directory in the root of out project. Then, we'll add it as a bindmount to our `util` container. This will enable us to leverage our local text editor to modify the chart artefacts. To do this we simply add the following ot our `docker-compose.yaml`:

```yaml
services: ...
volumes:
  - ${PWD}/service:/service
```

Next we'll creaete a `helm` chart. To do this, we'll exec back into the `util` container and run:

```console
helm create service
```

This will create a helm chart in our mounted `service` directory.

Once we've got our chart, we can clean up our templates directory by removing everything except the `_helpers.tlp` file. To quickly perform this, we exec into the `util` container and, navigate to `/service/templates` and run:

```console
ls | grep -v helpers | xargs rm -rf
```

# Templates

Next, we'll create all the necessary kubernetes artefacts and place them in the templates directory. We'll need the following:

- an `api-deployment.yaml` file, easily generated by running:

```console
kubectl create deployment api-deployment --image elijah668/api:1.0.0 -o yaml --dry-run=client > api-deployment.yaml
```

- an `api-service.yaml` file, easily generated by running:

```console
kubectl create service loadbalancer api-service --tcp=8000:8000 -o yaml --dry-run=client > api-service.yaml
```

- a `client-deployment.yaml` file, easily generated by running:

```console
kubectl create deployment client-deployment --image elijah668/client:1.0.0 -o yaml --dry-run=client > client-deployment.yaml
```

- an `client-service.yaml` file, easily generated by running:

```console
kubectl create service loadbalancer client-service --tcp=80:80 -o yaml --dry-run=client > client-service.yaml
```

- an `secret.yaml` file, easily generated by running:

```console

kubectl create secret generic secret --from-env-file .secret.conf -o yaml --dry-run=client >
 secret.yaml
```

**NOTE:** We need to create the .secret.conf environment file in advanced.

- an `config.yaml` file, easily generated by running:

```console
kubectl create configmap config --from-env-file=.conf -o yaml --dry-run=client > config.yaml
```

**NOTE:** We need to create the .conf environment file in advanced.

# Environment Variables:

We continue by adding the required environment variables in our deployment artefacts.

- in `api-deployment.yaml` we add:

```yaml
containers:
  - image: ...
    name: ...
    env:
      - name: MONGO_USER
        valueFrom:
          secretKeyRef:
            name: secret
            key: MONGO_USER
      - name: API_PORT
        valueFrom:
          configMapKeyRef:
            name: config
            key: API_PORT
      - name: MONGO_PASSWORD
        valueFrom:
          secretKeyRef:
            name: secret
            key: MONGO_PASSWORD
      - name: MONGO_DB_NAME
        valueFrom:
          secretKeyRef:
            name: secret
            key: MONGO_DB_NAME
```

- in `client-deployment.yaml` we add:

```yaml
containers:
  - image: ...
    name: ...
    env:
      - name: THEME
        valueFrom:
          configMapKeyRef:
            name: config
            key: THEME
```

# Add Values

Next we'll make our charts reusable by injecting dynamic parameters from the `values.yaml` file.

We begin by defining our `values.yaml` file:

```yaml
deployment:
  api:
    port: 8000
    target: 5000
    image: elijah668/api
    version: 1.0.0
    name: api
  client:
    port: 80
    image: elijah668/client
    version: 1.0.0
    name: client
    path: localhost
    theme: dark
```

We can now dynamically inject these values in our template files. The sintax for this is as follows.

- in `api-deployment.yaml`:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: {{ .Values.deployment.api.name }}
    ...
```

What we've done here, is to referene the `values.yaml` file and drill down to the `name` value on the deployment object. We can now proceed and make the rest of the `api-deployment.yaml` reusable by injecting all our `deployment.api` values in`api-deployment.yaml`:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: {{ .Values.deployment.api.name }}
  name: {{ .Values.deployment.api.name }}-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: {{ .Values.deployment.api.name }}
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: {{ .Values.deployment.api.name }}
    spec:
      containers:
        - image: {{ .Values.deployment.api.image }}:{{ .Values.deployment.api.version }}
          name: {{ .Values.deployment.api.name }}
          env:
            - name: MONGO_USER
              valueFrom:
                secretKeyRef:
                  name: secret
                  key: MONGO_USER
            - name: API_PORT
              valueFrom:
                configMapKeyRef:
                  name: config
                  key: API_PORT
            - name: MONGO_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: secret
                  key: MONGO_PASSWORD
            - name: MONGO_DB_NAME
              valueFrom:
                secretKeyRef:
                  name: secret
                  key: MONGO_DB_NAME
          resources: {}
status: {}
```

We continue by doing the same in `api-service.yaml`:

```yaml
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  labels:
    app: {{ .Values.deployment.api.name }}
  name: {{ .Values.deployment.api.name }}-service
spec:
  ports:
  - name: {{ .Values.deployment.api.port }}-{{ .Values.deployment.api.port }}
    port: {{ .Values.deployment.api.target }}
    protocol: TCP
    targetPort: {{ .Values.deployment.api.port }}
  selector:
    app: {{ .Values.deployment.api.name }}
  type: LoadBalancer
status:
  loadBalancer: {}
```

Moving on to `client-deployment.yaml`:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: {{ .Values.deployment.client.name }}
  name: {{ .Values.deployment.client.name }}-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: {{ .Values.deployment.client.name }}
  strategy: {}
  template:
    metadata:
      annotations:
        checksum/config: {{ include (print $.Template.BasePath "/config.yaml") . | sha256sum }}
      creationTimestamp: null
      labels:
        app: {{ .Values.deployment.client.name }}
    spec:
      containers:
        - image: {{ .Values.deployment.client.image }}:{{ .Values.deployment.client.version }}
          name: {{ .Values.deployment.client.name }}
          env:
            - name: THEME
              valueFrom:
                configMapKeyRef:
                  name: config
                  key: THEME
          resources: {}
status: {}
```

**NOTE:** We've also snuk in a `checsum annotation`:

```yaml
template:
  metadata:
    annotations:
      checksum/config:
        { { include (print $.Template.BasePath "/config.yaml") . | sha256sum } }
```

This enables rolling deployments whenever the our `config.yaml` changes. By default helm will not restart the pods if a configmap changes.

And finally, we inject our values in `client-service.yaml`:

```yaml
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  labels:
    app: {{ .Values.deployment.client.name }}
  name: {{ .Values.deployment.client.name }}-service
spec:
  ports:
  - name: {{ .Values.deployment.client.port }}-{{ .Values.deployment.client.port }}
    port: {{ .Values.deployment.client.port }}
    protocol: TCP
    targetPort: {{ .Values.deployment.client.port }}
  selector:
    app: {{ .Values.deployment.client.name }}
  type: LoadBalancer
status:
  loadBalancer: {}
```

# Control Flows

`Helm` provides us with special control structures to command the flow of a template generation. One such sructure is the `if / else` block, which allows us to conditionally including blocks of text in a template.

The syntax for this is as follows.

- in `config.yaml`:

```yaml
apiVersion: v1
data:
  API_PATH: {{ .Values.deployment.api.name }}
  API_PORT: "{{ .Values.deployment.api.port }}"
  {{- if .Values.deployment.client.theme }}
  THEME: {{ .Values.deployment.client.theme | quote }}
  {{- else }}
  THEME: "gold"
  {{- end }}
kind: ConfigMap
metadata:
  creationTimestamp: null
  name: config
```

What we've done here is to set conditionally set `THEME` to the value defined in our `values.yaml`, provided it exists. Conversely, we set it to `"gold"`.

**NOTE:**
This coupled with our `checksum annotation` allows us to have rolling deployments on changes in our `config.yaml` configmap. To see this in acion, we can change `deployments.client.theme` to `"dark"`, in our `values.yaml` and then run the following command in `/service`:

```consle
helm upgrade service .
```

Once our pods have restarted, we can go to `http://localhost:80` and expect to see the color scheme of our application get updated.

We can keep track of our pods status with:

```console
kubectl get pods -w
```

We ve reached the end. We can remove our helm deployment by running:

```console
helm uninstall service
```
